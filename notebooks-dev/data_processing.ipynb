{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing \n",
    "\n",
    "In this notebook, we will proceed to load the combined data generated in the previous step using the `data_extraction.ipynb` script. Our objective is to perform data processing to create a clean dataset that includes the `titles` (the feature of interest) and `token_title`. The purpose of this step is to produce an intermediate file that can be refined and utilized for various purposes.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for data processing\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retailer A data\n",
    "retA_data = pd.read_csv('../data/raw/retailerA.csv')\n",
    "# Retailer B data\n",
    "retB_data = pd.read_csv('../data/raw/retailerB.csv')\n",
    "# Combined data\n",
    "combined_data = pd.read_csv('../data/raw/combined_data_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Configuration\n",
    "\n",
    "Within this tokenizer setup, we incorporate the Spacy small English model. We perform several preprocessing steps, including the removal of capital letters, basic regex operations to eliminate special characters, and lemmatization based on user-defined parameters. Additionally, we exclude stop words during tokenization. This tokenizer will subsequently be integrated into the vectorization process.\n",
    "\n",
    "This function is saved as a module in `..scripts/text_tokenization.py` for easy renderization and usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load('en_core_web_sm') # Load the spacy small English model \n",
    "re_pattern = re.compile(r'[\\W_]+') # Compile the regular expression to use\n",
    "\n",
    "def tokenizeText(text: str, lemmas: bool) -> str:\n",
    "    \"\"\"\n",
    "    Tokenizes the input text and returns a processed string.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to be tokenized.\n",
    "        lemmas (bool): If True, the function returns lemmatized tokens; if False, it returns regular tokens.\n",
    "\n",
    "    Returns:\n",
    "        str: A processed string containing tokens from the input text.\n",
    "\n",
    "    Example:\n",
    "    >>> tokenizeText(\"This is an example sentence.\", True)\n",
    "    'example sentence'\n",
    "    \"\"\"\n",
    "\n",
    "    text = re_pattern.sub(' ', text) # Use the compiled regex pattern\n",
    "\n",
    "    # Tokenization\n",
    "    doc = spacy_nlp(text)\n",
    "    if lemmas:\n",
    "        tokens = [token.lemma_.lower() for token in doc if token.lemma_.lower() not in STOP_WORDS]\n",
    "        # Rejoin tokens into a single string\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        tokens = [token.text.lower() for token in doc if token.text.lower() not in STOP_WORDS]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of analysis and record-keeping, we will create and save three intermediate datasets. These datasets will not only contain the original data but will also feature an additional column named `title_token` in the DataFrame. This column will store the results of tokenization.\n",
    "\n",
    "**Note:** For this particular application, since the `title` is not including verbs or words subject to lemmantization, we will be setting this parameter as false. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column in dataset with tokenized 'title'\n",
    "retA_data['title_token'] = retA_data['title'].apply(tokenizeText, args=(False,))\n",
    "retB_data['title_token'] = retB_data['title'].apply(tokenizeText, args=(False,))\n",
    "combined_data['title_token'] = combined_data['title'].apply(tokenizeText, args=(False,))\n",
    "\n",
    "# Saving intermedite datasets\n",
    "retA_data[['title', 'title_token']].to_csv('../data/processed/retailerA_tokens.csv')\n",
    "retB_data[['title', 'title_token']].to_csv('../data/processed/retailerB_tokens.csv')\n",
    "combined_data[['title', 'title_token']].to_csv('../data/processed/combined_data_tokens.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **AI tool usage for this notebook**\n",
    "\n",
    "#### ChatGPT 3.5\n",
    "* Improving markdown annotations and function doctrings\n",
    "\n",
    "#### ChatBPT 4\n",
    "* Providing regex expressions for different purposes\n",
    "* Help with the apply method for the tokenizeText function call\n",
    "* Improving modularity in repository structure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
